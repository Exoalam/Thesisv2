{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "আমি একজন ছাত্র।\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from normalizer import normalize \n",
    "\n",
    "model_g = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "model_b_e = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")\n",
    "model_e_b = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\")\n",
    "tokenizer_g = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer_b_e = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\", use_fast=False)\n",
    "tokenizer_e_b = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\", use_fast=False)\n",
    "\n",
    "input_sentence = \"আমার নাম নাফিউল। আমি এশিয়ায় থাকি।\"\n",
    "input_ids = tokenizer_b_e(normalize(input_sentence), return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_b_e.generate(input_ids)\n",
    "decoded_tokens = tokenizer_b_e.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "input_text = decoded_tokens\n",
    "input_ids = tokenizer_g(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model_g.generate(input_ids, max_length=70)\n",
    "decoded_tokens = tokenizer_g.decode(outputs[0], skip_special_tokens=True)\n",
    "input_sentence = decoded_tokens\n",
    "input_ids = tokenizer_e_b(normalize(input_sentence), return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_e_b.generate(input_ids)\n",
    "decoded_tokens = tokenizer_e_b.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(decoded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a program running on a computer, so I don't have feelings, but I'm here and ready to help you with any questions or information you need! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square root of x is the cube root of y. If x = 6, what is the stroke of 2?\n",
      "2\n",
      "২\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from normalizer import normalize\n",
    "\n",
    "\n",
    "\n",
    "model_g = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "model_b_e = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")\n",
    "model_e_b = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\")\n",
    "tokenizer_g = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer_b_e = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\", use_fast=False)\n",
    "tokenizer_e_b = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\", use_fast=False)\n",
    "\n",
    "        # Normalize and translate input sentence to English\n",
    "input_sentence = \"x এর বর্গমূল হল y এর ঘনমূল। x = 6  হলে 2 এর ঘাত y কত?\"\n",
    "normalized_input = normalize(input_sentence)\n",
    "input_ids = tokenizer_b_e(normalized_input, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_b_e.generate(input_ids)\n",
    "decoded_tokens = tokenizer_b_e.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(decoded_tokens)\n",
    "input_text = decoded_tokens\n",
    "        # Generate text with model_g\n",
    "input_ids = tokenizer_g(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model_g.generate(input_ids, max_length=70)\n",
    "decoded_tokens = tokenizer_g.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_tokens)\n",
    "        # Translate the generated text back to the original language\n",
    "normalized_output = normalize(decoded_tokens)\n",
    "input_ids = tokenizer_e_b(normalized_output, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_e_b.generate(input_ids)\n",
    "decoded_tokens = tokenizer_e_b.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(decoded_tokens)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from normalizer import normalize\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "model_b_e = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\")\n",
    "model_e_b = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\")\n",
    "tokenizer_b_e = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_bn_en\", use_fast=False)\n",
    "tokenizer_e_b = AutoTokenizer.from_pretrained(\"csebuetnlp/banglat5_nmt_en_bn\", use_fast=False)\n",
    "\n",
    "\n",
    "        # Normalize and translate input sentence to English\n",
    "input_sentence = \"x এর বর্গমূল হল y এর ঘনমূল। x = 6  হলে 2 এর ঘাত y কত?\"\n",
    "normalized_input = normalize(input_sentence)\n",
    "input_ids = tokenizer_b_e(normalized_input, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_b_e.generate(input_ids)\n",
    "decoded_tokens = tokenizer_b_e.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "input_variable = decoded_tokens\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly and knowledgeable chatbot who enjoys explaining complex topics in an easy-to-understand manner.\"},\n",
    "    {\"role\": \"user\", \"content\": input_variable}\n",
    "  ],\n",
    "  max_tokens=20\n",
    ")\n",
    "decoded_tokens = completion.choices[0].message[\"content\"]\n",
    "normalized_output = normalize(decoded_tokens)\n",
    "input_ids = tokenizer_e_b(normalized_output, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = model_e_b.generate(input_ids)\n",
    "decoded_tokens = tokenizer_e_b.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(decoded_tokens)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
